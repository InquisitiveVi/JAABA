\documentclass[10pt, onecolumn]{article}
 \usepackage{color,multirow,multicol}
 \usepackage{amsmath, amssymb, amsthm}
 \usepackage{calc,geometry,mathrsfs,xspace}
\usepackage{algorithm, algorithmic}

 \setlength{\evensidemargin}{0.0in}
\setlength{\oddsidemargin}{0.0in} \setlength{\textwidth}{6.5in}
\setlength{\topmargin}{-0.5in} \setlength{\textheight}{9in}
% type user-defined commands here

  \makeatletter
 \DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
 \def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}
 \newcommand{\R}{\mathbb{R}}
 \newcommand{\Li}{\mathcal{L}}
 \newcommand{\1}{\textbf{1}}
 \newcommand{\err}{\epsilon}
  \def\etal{\emph{et al}\onedot}



\title{Behavior Segmentation: Training, Inference, Features, and Loss Functions}
\author{Steve Branson\\sbranson@cs.ucsd.edu 
\and Kristin Branson\\bransonk@janelia.hhmi.org}

\begin{document}
%{\newpage \null \vskip 2em \begin{center} {\LARGE Behavior Segmentation: Training, Inference, Features, and Loss Functions \par} \end{center} \par}
\maketitle

\section{Introduction}
We present a behavior detection and segmentation learning algorithm using a structural SVM.  Other methods based on per-frame behavior classification tend to erroneously produce many short bouts of behavior.  This can be undesirable, as the true objective in many practical applications is related to counting the number of bouts of each behavior.  It is common for such methods to apply a post-processing smoothing phase (e.g., using a Hidden Markov Model) to help remedy this problem.  On the other hand, since the behavior segmentation problem is one-dimensional (as opposed to the 2D image segmentation problem), algorithms that solve more sophisticated formulizations of the segmentation problem optimally are possible and computationally efficient.   

The structural SVM approach learns a function to directly predict a behavior segmentation, and can optimize more appropriate loss functions, such as the number of bouts misclassified.  Additionally, it can optimize over more appropriate higher order statistics of the predicted segmentation, such as the mean, standard deviation, min, and max feature response of each bout, and comparisons of feature responses to neighboring bouts.  Since the learning problem is convex, performance is predictable and computationally efficient, even when using feature spaces with tens of thousands of features.  Since it is relatively easy to adapt the method to different types of loss functions and incorporate complex high-dimensional feature spaces with very few parameters or hand-tuning necessary, the method should be general to a wide variety of different animals or types of behaviors.

\section{Training}

Let $x_i$ be an observed video sequence of $T_i$ frames, $x_i=\{x_i^1,x_i^2,...,x_i^{T_i}\}$.  Here, each $x_i^t$ is associated with a vector of frame-level features $\vec{\phi}(x_i^t)$.  $\vec{\phi}(x_i^t)$ could be a collection of parameters returned by a tracker, a collection of appearance parameters of the $t$-th frame, or some combination of the two.

Let $y_i$ be the corresponding groundtruth behavior segmentation.  A behavior segmentation is a collection of bouts of behavior $y_i=\{y_i^1,y_i^2,...,y_i^{m_i}\}$, where each $y_i^j=(s_i^j,e_i^j,b_i^j)$ is a bout that begins at time $s_i^j$ and ends at time $e_i^j$.  $b_i^j \in B$ is the label of the $j$-th bout, where $B$ is a set of mutually exclusive behavior classes (behaviors that cannot occur at the same time).

Our goal is to learn a computationally efficient behavior segmentation function $g:x \to y$ given a training set of $n$ labelled sequences $D=\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\}$.  We train the algorithm using a structural SVM, which can be understood as an extension of techniques used for binary classfication to problems with more complicated structures and loss functions.  It could also be understood as a restriction of graphical models to certain representations that admit efficient convex learning algorithms.

The main ingredients of the learning algorithm are:
\begin{itemize} 
 \item \textbf{A customizable loss function} $\mathcal{L}(y,\hat{y})$, which is a penalty incurred for predicting segmentation $\hat{y}$ when the true segmentation is $y$
 \item \textbf{A customizable feature space} $\vec{\psi}_{seg}(x,y)$ that converts frame-level features $\vec{\phi}(x_i^t)$ into a more semantically meaningful representation that is a function of the predicted segmentation $y$
 \item \textbf{A score function} $f:x,y \to \mathcal{R}$ that is high when $y$ is a good segmentation for $x$ and low when it is a bad segmentation.  For computational purposes, $f$ is parameterized by a set of linear weights $\vec{w}$
\begin{equation}
 f(x,y) = \vec{w} \cdot \vec{\psi}_{seg}(x,y)
\end{equation}
 \item \textbf{An inference algorithm} that is capable of efficiently finding the highest scoring segmentation out of the space of all possible segmentations
\begin{equation}
 g(x) = \arg\max_{y} f(x,y)
\end{equation}
 \item \textbf{A learning algorithm} that finds the optimal set of parameters $\vec{w}$ over the training set $D$ by solving
\begin{eqnarray}
\vec{w}^* = \arg\min_{\vec{w}} \|\vec{w}\|^2 + C \sum_{i=1}^n \left( \max_{\hat{y}} \left[ f(x_i,\hat{y}) + \mathcal{L}(y,\hat{y}) \right] - f(x_i,y_i) \right)
\label{eq:objective}
\end{eqnarray}
\end{itemize}

The objective in Equation \ref{eq:objective} is minimized if for every training example $x_i$, the score of the true segmentation $y_i$ is higher than the score of any other segmentation $\hat{y}$ by at least $\mathcal{L}(y,\hat{y})$.  This is called margin-scaling, and has some theoretical justifications based on large-margin theory.  For the 0/1 loss $\mathcal{L}(y,\hat{y})=1[y \ne \hat{y}]$, the objective is identical to a multiclass SVM.  

Equation \ref{eq:objective} is convex in $\vec{w}$ because it is the maximum of a set of affine functions.  This holds for any arbitrary choice of $\mathcal{L}(y,\hat{y})$ and $\vec{\psi}_{seg}(x,y)$ (as long as they are computable).  The main caveat is that it involves taking the $\max$ over an exponentially large space of segmentations.  In practice though, most of these segmentations are similar or redundant.  The optimization algorithm that the SVM$^{struct}$ package is a delayed constraint generation algorithm that iteratively samples new segmentations by searching for the most violated constraint
\begin{equation}
\bar{y} = \arg\max_{\hat{y}} \left[ f(x_i,\hat{y}) + \mathcal{L}(y,\hat{y}) \right]
\label{eq:most_violated}
\end{equation}

\begin{algorithm}[t]
\caption{Structural SVM Learning}
INPUT: $D=\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\}$
\begin{algorithmic}[1]
\STATE Initialize: $\vec{w}^* \gets \vec{0},\ \ \forall i\ A_i \gets \emptyset$ 
\REPEAT
\FOR{i=1 to n}
\STATE Add the most violated constraint to the active set: $A_i \gets A_i \cup \max_{\hat{y}} \left[ f(x_i,\hat{y}) + \mathcal{L}(y,\hat{y}) \right]$ 
\ENDFOR
\STATE Minimize weights over active sets: $\vec{w}^* \gets \arg\min\limits_{\vec{w}} \|\vec{w}\|^2 + C \sum\limits_{i=1}^n \left( \max\limits_{\hat{y} \in A_i} \left[ f(x_i,\hat{y}) + \mathcal{L}(y,\hat{y}) \right] - f(x_i,y_i) \right)$ 
\UNTIL{Until convergence}
\end{algorithmic}
OUTPUT: $\vec{w}^*$
\end{algorithm}

The basic method is similar to the pseudo-code shown in Algorithm 1.  Due to the inherent structure in related segmentations, the number of segmentations that need to be sampled in practice is small.  In our experiments, we typically gain another bit of precision toward reaching the global optimum in a constant number of iterations (in our case ~50), which is consistent with optimization theory.  This factor of 50 depends on the amount of structural similarity existing in the labels $y$.

\section{Loss Function}
We use the bout-level loss function 
\begin{eqnarray}
\mathcal{L}(y,\hat{y}) = \sum\limits_{(s,e,b) \in y} \frac{\ell^b_{fn}}{e-s} \bigg( \bigcap_{\hat{y},\hat{b} \ne b}(s,e) \bigg) + \sum\limits_{(\hat{s},\hat{e},\hat{b}) \in \hat{y}} \frac{\ell^b_{fp}}{\hat{e}-\hat{s}} \bigg( \bigcap_{y,b \ne \hat{b}}(\hat{s},\hat{e}) \bigg) 
\label{eq:loss_fp}
\end{eqnarray}
where $\bigcap_{(\hat{s},\hat{e},\hat{b}) \in \hat{y},\hat{b} \ne b}(s,e)$ is the number of frames in $\hat{y}$ intersecting with $(s,e)$ with different behavior class $\hat{b} \ne b$, $\ell^b_{fn}$ is the cost for missing a bout of class $b$, and  behavior $\ell^{\hat{b}}_{fp}$ is the cost for incorrectly detecting a bout of class $\hat{b}$.  This loss function softly penalizes predictions where the start or end of the bout is slightly incorrect.  On the other hand, it is very different than a per-frame cost, due to the addition of a false positive cost and because it penalizes bouts of different duration equally.  As a result, $\mathcal{L}(y,\hat{y})$ heavily favors smooth segmentations that can explain the observed data using a small number of bouts.

A generalized version of $\mathcal{L}(y,\hat{y})$ that supports arbitrary confusion costs can be written as
\begin{eqnarray}
\mathcal{L}'(y,\hat{y}) = \sum\limits_{(s,e,b) \in y} \sum\limits_{b'\in B} \frac{\ell^b_{\hat{b}}}{e-s} \bigg( \bigcap_{\hat{y},\hat{b}=b'}(s,e) \bigg) +  \sum\limits_{(\hat{s},\hat{e},\hat{b}) \in \hat{y}} \sum\limits_{b' \in B} \frac{\ell^{\hat{b}}_b}{\hat{e}-\hat{s}} \bigg( \bigcap_{y,b=b'}(\hat{s},\hat{e}) \bigg)
\label{eq:loss_conf}
\end{eqnarray}
where $\ell^b_{\hat{b}}$ is the cost of predicting a bout of class $\hat{b}$ when the true class is $b$.

\section{Score Function}

We can decompose the segmentation score function $f(x,y)$ in terms of bout scores and transition costs, summing over each bout in the segmentation:
\begin{eqnarray}
 f(x,y) = \sum_{(s^j,e^j,b^j) \in y} \left[f_{b}(x^j,s^j,e^j) + h(b^j,b^{j+1}) \right], &&\mathrm{Segmentation\ Score}\\
\label{eq:score_decomposition}
 f_{b}(x,s,e) = \vec{w}_b \cdot \vec{\psi}_{bout}(x,s,e,b)], &&\mathrm{Bout\ Score}\\
 h(b^j,b^{j+1}) = \vec{\lambda}_{b^j} \cdot \vec{\psi}_{trans}(b^j,b^{j+1}), &&\mathrm{Bout\ Transition\ Score}
\end{eqnarray}
Here, $f_{b}(x,s,e)$ is a bout scoring function that is high when a bout of behavior of class $b$ is likely to occur between time frames $s$ and $e$.  $\vec{\psi}_{bout}(x,s,e,b)$ is a bout-level feature space, which is customizable (see next section).  $h(b^j,b^{j+1})$ is a (usually negative) score associated with transitioning from behavior $b^j$ to behavior $b^{j+1}$.  Setting $\vec{\psi}_{trans}(b^j,b^{j+1})$ to a length $|B|$ vector of zeros, where the $b^{j+1}$th entry is -1 allows $\vec{\lambda}_b$ to be interpreted as a vector of behavior transition costs.

Algorithm 1 will jointly learn all bout score parameters $\vec{w}_b$ and transition costs $\vec{\lambda}_b$ if we write the expressions for $\vec{w}$ and $\vec{\psi}_{seg}(x,y)$ as
\begin{eqnarray}
\vec{w} = [\vec{w}_1,\vec{\lambda}_1,\vec{w}_2,\vec{\lambda}_2,...,\vec{w}_{|B|},\vec{\lambda}_{|B|}]\\
\vec{\psi}_{seg}(x,y) = [u_1,v_1,u_2,v_2,...,u_{|B|},v_{|B|}]\\
u_k=\sum_{\substack{(s^j,e^j,b^j)\in y\\b^j=k}} \vec{\psi}_{bout}(x,s^j,e^j,b^j),\ \ \ \ v_k=\sum_{\substack{(s^j,e^j,b^j)\in y\\b^j=k}}\vec{\psi}_{trans}(b^j,b^{j+1})
\end{eqnarray}

\section{Feature Space}
The feature space $\vec{\psi}_{seg}(x,y)$ is a function of both the raw input $x$ and candidate segmentation $y$.  It is an abstracted version of the feature space, where statistical patterns existing in $x$ may be more easily discovered or represented using knowledge of the start, end, and behavior class of each bout.  We define a variety of different types of basic feature expansion operations that are used to synthesize bout-level features $\vec{\psi}_{bout}(x,s,e,b)$ from frame-level features $\vec{\phi}(x_i^t)$.  It is our hope that a wide variety of different types of behaviors can be represented and learned using combinations of these different feature expansion operations.  All bout-level features are computable in constant time (independent of the duration of the bout) using precomputed data structures like the integral images.

\subsection{Bout Statistic Features}
These are features that compute some simple statistic over the bout duration.  Each particular statistic occurs over a single frame feature $\phi_k(x^t)$ at a time, and adds one new bout-level feature to $\vec{\psi}_{seg}(x,y)$.  Sum, average, and standard deviation features are computed using precomputed integral images over the raw feature and and square feature responses.  Min and max features are updatable in constant time.

%{\scriptsize 
\begin{tabular}{ |c|c|c| }
  \hline
  Sum: & Sum Variance: & Minimum: \\
  $\mathrm{sum}_k(s,e)=\sum\limits_{s \le t < e} \phi_k(x^t)$ & $\mathrm{var}_k(s,e) = \sum\limits_{s \le t < e} \left(\phi_k(x^t)-\mu_k(s,e)\right)^2$ & $\mathrm{min}_k(s,e)=\min\limits_{s \le t < e} \phi_k(x^t)$ \\
  \hline
  Average: & Standard Deviation: & Maximum: \\
  $\mu_k(s,e) = \frac{1}{e-s}\sum\limits_{s \le t < e} \phi_k(x^t)$ & $\sigma_k(s,e)=\sqrt{\frac{1}{e-s}\mathrm{var}_k(s,e)}$ & $\mathrm{max}_k(s,e)=\max\limits_{s \le t < e} \phi_k(x^t)$ \\
  \hline
\end{tabular}
%}

\subsection{Threshold and Histogram Features}
We allow threshold features, which convert any of the above bout statistics to features of value 0 or 1:
\begin{eqnarray*}
 \mathrm{Max\ Threshold:\ \ } &\mathrm{gt}_k(s,e,Q,\mathrm{op}) = [\mathrm{g}_k^1,\mathrm{g}_k^2,...,\mathrm{g}_k^Q],\ \ \ \ &\mathrm{g}_k^l=1[q_k^l > \mathrm{op}_k(s,e)]\\
 \mathrm{Min\ Threshold:\ \ } &\mathrm{lt}_k(s,e,Q,\mathrm{op}) = [\mathrm{l}_k^1,\mathrm{l}_k^2,...,\mathrm{l}_k^Q],\ \ \ \ &\mathrm{l}_k^l=1[q_k^l < \mathrm{op}_k(s,e)]
\end{eqnarray*}
where $\mathrm{op}_k$ is one of the above bout statistics (e.g., $\mathrm{sum}_k$, $\mu_k$, $\sigma_k$, etc.) and each $q_k^l$ is an arbitrary constant.  The threshold constants are computed by evaluating median statistics on the training set.  In other words, we construct a set of $Q$ thresholds $q_k^1,q_k^2,...q_k^Q$, such that an equal fraction of the training set lies in each interval $q_k^{l}$ to $q_k^{l+1}$.  

We also allow raw histogram features, which assign a frame to histogram bin $l$ if $q_k^l \le \phi^k(x^t) < q_k^{l+1}$, and computes a count of the number of frames lying in each histogram bin: 
\begin{eqnarray*}
 \mathrm{Histogram:\ \ } && \mathrm{hist}_k(s,e,Q)=[h_k^1,h_k^2,...,h_k^Q],\ \ \ \ \ \ h_k^l = \sum\limits_{s \le t < e} 1[q_k^l \le \phi_k(x^t) < q_k^{l+1}]\\
 \mathrm{Normalized\ Histogram:\ \ } && \mathrm{n\_hist}_k(s,e,Q) = \frac{1}{e-s}[h_k^1,h_k^2,...,h_k^Q]
\end{eqnarray*}

\subsection{Temporal Region Features}
A temporal feature is a feature occurring over some temporal region that may be different than the entire bout $(s,e)$.  It is usually convenient to express the region in a coordinate system normalized with respect to the start and end of the bout, where the point $s$ maps to 0 and $e$ maps to 1.

Temporal partition features divide the bout into $R$ evenly spaced temporal regions and add new features to $\vec{\psi}_{bout}(x,s,e,b)$ as the concatenation of each region.  Temporal pyramids concatenate multiple partitions of size $R=2^0,2^1,...2^{L-1}$.
\begin{eqnarray*}
 \mathrm{Temporal\ Region:\ \ } &&\mathrm{op'}_k(s,e,s',e') = \mathrm{op}_k\bigg(s+s'(e-s),\ s+e'(e-s)\bigg)\\
 \mathrm{Temporal\ Partition:\ \ } &&\mathrm{part}_k(s,e,R,\mathrm{op}) = [r_k^1,r_k^2,...r_k^R],\ \ \ \ \ r_k^l = \mathrm{op'}_k\bigg(s,e,\frac{l-1}{R},\frac{l}{R} \bigg)\\
 \mathrm{Temporal\ Pyramid:\ \ } &&\mathrm{pyr}_k(s,e,L,\mathrm{op}) = [p_k^1,p_k^2,...p_k^L],\ \ \ \ \ \ p_k^l=\mathrm{part}_k(s,e,2^{l-1})
\end{eqnarray*}
Temporal region, partition, and pyramid features can be constructed ontop of bout statistic, threshold, or histogram features.

\subsection{Temporal Difference Features}
We use Haar-like features, which are the difference between two temporal region features.  In particular, we use features designed to accentuate the difference between frames in the beginning of the bout and the frames preceeding the bout, and the difference between frames in the end of the bout and frames following the bout:
\begin{eqnarray*}
 \mathrm{Start\ Difference:\ \ } &&\mathrm{start}_k(s,e,d,\mathrm{op}) = \mathrm{op'}_k(s,e,0,d) - \mathrm{op'}_k(s,e,-d,0) \\
 \mathrm{Absolute\ Start\ Difference:\ \ } &&\mathrm{a\_start}_k(s,e,d,\mathrm{op}) = |\mathrm{start}_k(s,e,d,\mathrm{op})| \\
 \mathrm{End\ Difference:\ \ } &&\mathrm{end}_k(s,e,d,\mathrm{op}) = \mathrm{op'}_k(s,e,1-d,1) - \mathrm{op'}_k(s,e,1,1+d)\\
 \mathrm{Absolute\ End\ Difference:\ \ } &&\mathrm{a\_end}_k(s,e,d,\mathrm{op}) = |\mathrm{end}_k(s,e,d,\mathrm{op})| 
\end{eqnarray*}

\subsection{Harmonic Features}
Harmonic features accentuate periodic, repeated motions such as waving or walking:
\begin{eqnarray*}
  \mathrm{Harmonic:\ \ } && \mathrm{harmonic}_k(s,e,h,\mathrm{op})   = \sum_{l=1}^H -1^l \times \mathrm{op'}_k\bigg(s,e,\frac{l-1}{h},\frac{l}{h} \bigg)\\
  \mathrm{Absolute\ Harmonic:\ \ } &&\mathrm{a\_harmonic}_k(s,e,h,\mathrm{op}) = |\mathrm{harmonic}_k(s,e,h,\mathrm{op})|
\end{eqnarray*}

\subsection{Global Difference Features}
Global difference features compare the feature response inside the bout to the global average response (the average response throughout the entire tracked sequence):
\begin{eqnarray*}
  \mathrm{Global\ Sum\ Difference:\ \ } &&\mathrm{g\_sum}_k(s,e,\mathrm{op})  = \mathrm{op}_k(s,e) - \frac{\mathrm{op}_k(0,T)}{T}(e-s)\\
  \mathrm{Global\ Absolute\ Sum\ Difference:\ \ } && \mathrm{g\_a\_sum}_k(s,e,\mathrm{op}) = |\mathrm{g\_sum}_k(s,e,\mathrm{op})| \\
  \mathrm{Global\ Average\ Difference:\ \ } &&\mathrm{g\_ave}_k(s,e,\mathrm{op})  = \frac{\mathrm{op}_k(s,e)}{e-s} - \frac{\mathrm{op}_k(0,T)}{T}\\
  \mathrm{Global\ Absolute\ Average\ Difference:\ \ } && \mathrm{g\_a\_ave}_k(s,e,\mathrm{op}) = |\mathrm{g\_ave}_k(s,e,\mathrm{op})| 
\end{eqnarray*}

\section{Inference}
We can find the optimal solution to $g(x) = \mathrm{argmax}_{y} f(x,y)$ where $f(x,y)$ is defined in Equation \ref{eq:score_decomposition} using dynamic programming in time O($T^2 (B^2 + D)$), where $T$ is the number of frames in $x$, $B$ is the number of behavior classes, and $D$ is the dimensionality of the bout-level feature space $\vec{\psi}_{seg}(x,y)$.  The algorithm is shown in Algorithm 2.

\begin{algorithm}[t]
\caption{Behavior Segmentation}
INPUT: $x$, $\vec{w}$
\begin{algorithmic}[1]
\STATE Initialize: $\mathrm{S}[0][1...B] \gets 0$, $\mathrm{M}[0][1...B] \gets \emptyset$
\FOR{$e=1$ to $T$}
\FOR{$b \in B$}
\STATE Compute the optimal score $S[e][b]$ and solution $M[e][b]$ for beginning a bout of class $b$ at time $e$:\\
{\flushright $\mathrm{S}[e][b],\mathrm{M}[e][b] \gets \min\limits_{0 \le s<e,b' \in B} \left( \mathrm{S}[s][b'] + \vec{w}_b \cdot \vec{\psi}_{bout}(x,s,e,b) - \lambda_{b'}(b) \right)$ \\}
\ENDFOR
\ENDFOR
\STATE Extract optimal solution: $(s^m,b^m) \gets \max\limits_b M[T][b],\ \ \ \ \ \ (s^{i-1},b^{i-1}) \gets M[s^{i}][b^{i}], \ \ \ \ i=m,m-1,...2$ 
\end{algorithmic}
OUTPUT: $y=\{(s^1,e^1,b^1),(s^2,e^2,b^2),...,(s^m,e^m,b^m)\}$
\end{algorithm}

The algorithm to find the most violated constraint (Eq. \ref{eq:most_violated}) is identical, except that Line 4 of Algorithm 2 incorporates the appropriate component of the loss term
\begin{eqnarray}
 \mathrm{S}[e][b],\mathrm{M}[e][b] \gets \min\limits_{s<e,b' \in B} \left( \mathrm{S}[s][b'] + \vec{w}_b \cdot \vec{\psi}_{bout}(x,s,e,b) - \lambda_{b'}(b) + \mathcal{L}_{comp}(y,(s,e,b')) \right) \\
 \mathcal{L}_{comp}(y,(\bar{s},\bar{e},\bar{b})) = \sum\limits_{(s,e,b) \in y} \bigg( \frac{\ell^b_{\bar{b}}}{e-s} + \frac{\ell^{\bar{b}}_b}{\bar{e}-\bar{s}} \bigg) \bigg( (s,e) \cap (\bar{s},\bar{e}) \bigg)
\end{eqnarray}

We can improve the runtime of Algorithm 2 to O$\big(T \frac{\log T}{\log (1+\gamma)} (B^2 + D)\big)$ by reducing the search space in line 4 to time durations in a geometrically increasing series $(1+\gamma)^i$, and $\gamma$ is an arbitarily small approximation factor.

\section{Active Labeling}

We allow an active labeling interface, where the user can speed up the annotation process using an initial segmenter trained on only a small training set.  This is done by running the segmentation algorithm, correcting the mistakes by providing a partial labelling $y'$ (an assignment to only a fraction of the frames), re-running the segmentation algorithm such that any returned segmentation must agree with $y'$
\begin{equation}
g(x) = \arg\max_{y, y' \subseteq y} f(x,y)
\label{eq:active_labelling}
\end{equation}
and so on.  This can be implemented with a slight change to Line 4 of Algorithm 2
\begin{eqnarray}
 \mathrm{S}[e][b],\mathrm{M}[e][b] \gets \min\limits_{s<e,b' \in B} \left( \mathrm{S}[s][b'] + \vec{w}_b \cdot \vec{\psi}_{bout}(x,s,e,b) - \lambda_{b'}(b) + \delta(y',s,e,b) \right) \\
 \delta(y',s,e,b) = \left\{ \begin{array}{ll}  -\infty & \mathrm{if\ }\bigcap\limits_{y',b' \ne b} (s,e) > 0\\0 & \mathrm{otherwise} \end{array} \right.
\end{eqnarray}

Additionally, as new training examples are added, we can reduce the re-training time significantly by caching the active sets $A_i$ from prior runs of the algorithm.


\end{document}
